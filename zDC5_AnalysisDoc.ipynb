{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Insight Data Challenge 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "There are six functions in my solution:\n",
    "1. decode_sort() - takes an input and output filename, and calls decode_str to do the work on the input\n",
    "2. decode_str() - takes a string as described in the challenge description and returns it cleaned and sorted\n",
    "3. remove_bad_characters() - cleans a single word or number\n",
    "4. tokenize() - returns a list of strings separated by spaces\n",
    "5. sort_integers() - sorts numbers from least to greatest\n",
    "6. sort_words() - sorts words alphabetically\n",
    "\n",
    "The last three of these (tokenize, sort_integers, and sort_words) are simple wrappers for python functions. As might be expected, these perform the best almost regardless of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging into my code\n",
    "The first function, decode_sort, really just reads and writes to input/output files. There isn't a lot of optimization that could really happen here.<br><br>\n",
    "decode_str() is the switchboard of the operation, keeping track of all the lists of strings, sending them to be cleaned and then sorted, and then putting them back together in the proper order. This is where my code takes up space, as it creates three new lists: one list of words, one of numbers, and one of the order in which the tokens should appear. This is simple and understandable, but it obviously grows with the number of strings in the original input. We could probably implement an in-line version, if space was a concern (though I imagine we would have to sort through the original input several times in order to do this, so time complexity might grow).<br><br>\n",
    "According to cProfile.runctx(), remove_bad_characters() is the place where my code spends most of its time. This is unsurprising, as I'm enumerating through every character and checking each to see if it is alphanumeric before returning the cleaned string. There are a couple of ways that I could imagine improving this function:\n",
    "1. Fail fast - we could check whether the entire string is already clean before we start examining each character; if it's already clean, we could simply return it without loking at any characters.\n",
    "2. Dynamic programming - if we expect a significant number of dubplicates in our input string, we could store the strings that we have previously cleaned, and if we come across one, simply return the cleaned string without going through the work of examining each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
